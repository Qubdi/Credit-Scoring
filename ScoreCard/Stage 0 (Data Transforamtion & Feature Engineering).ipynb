{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-22T08:42:33.616052800Z",
     "start_time": "2025-08-22T08:42:30.984847500Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ScoringPy import Processing\n",
    "from pathlib import Path\n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpd\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mScoringPy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Processing\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpathlib\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Path\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\Credit-Scoring\\venv\\Lib\\site-packages\\ScoringPy\\__init__.py:4\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# Import classes and functions from library\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mScoringPy\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodule\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m *\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# Make these modules available in the global scope\u001B[39;00m\n\u001B[32m      8\u001B[39m __all__ = [\u001B[33m'\u001B[39m\u001B[33mWoeAnalysis\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mProcessing\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mCreditScoring\u001B[39m\u001B[33m'\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\Credit-Scoring\\venv\\Lib\\site-packages\\ScoringPy\\module.py:3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpipeline\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Pipeline\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpathlib\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Path \u001B[38;5;28;01mas\u001B[39;00m pth\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmatplotlib\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mplt\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mseaborn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msns\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpd\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T17:34:55.910708Z",
     "start_time": "2024-11-14T17:34:55.899125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "current_path = Path.cwd()\n",
    "RowData_path = f\"{current_path.parent}\\\\Data\\\\RowData\\\\\"\n"
   ],
   "id": "b7f7f7c6947031b9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T17:43:39.240443Z",
     "start_time": "2024-11-14T17:43:39.211934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_applications():\n",
    "    # Load the applications data from a Feather file located at RowData_path\n",
    "    data = pd.read_feather(f\"{RowData_path}applications.feather\")\n",
    "\n",
    "    # Calculate the complicity amount: difference between goods price and loan amount\n",
    "    data[\"complicity_amount\"] = data[\"goods_price\"] - data[\"loan_amount\"]\n",
    "\n",
    "    # Calculate the complicity percentage: (complicity amount / loan amount) * 100\n",
    "    data[\"complicity_percentage\"] = (data[\"complicity_amount\"] / data[\"loan_amount\"]) * 100\n",
    "\n",
    "    # Count the number of non-null external source scores for each row\n",
    "    # The scores are from ext_source_score_1, ext_source_score_2, and ext_source_score_3\n",
    "    data['ext_source_count'] = data[[\"ext_source_score_1\", \"ext_source_score_2\", \"ext_source_score_3\"]].notnull().sum(axis=1)\n",
    "\n",
    "    # Return the processed DataFrame\n",
    "    return data\n",
    "\n",
    "def process_contacts():\n",
    "    # Load the contacts data from a Feather file located at RowData_path\n",
    "    data = pd.read_feather(f\"{RowData_path}contacts.feather\")\n",
    "\n",
    "    # Calculate the total number of mobile contact methods available for each row\n",
    "    # This sums the values of mobile, emp_phone, work_phone, and phone columns\n",
    "    data['contact_mobile_count'] = data[[\"mobile\", \"emp_phone\", \"work_phone\", \"phone\"]].apply(lambda x: x.sum(), axis=1)\n",
    "\n",
    "    # Return the processed DataFrame\n",
    "    return data\n",
    "\n",
    "def process_credit_info():\n",
    "\n",
    "    def overdue_days():\n",
    "\n",
    "        # Group by 'loan_id' and 'credit_status' to calculate the required stats\n",
    "        loan_summary = data.groupby(['loan_id', 'credit_status'])['overdue_days'].agg(\n",
    "            sum_overdue_days='sum',\n",
    "            max_overdue_days='max'\n",
    "        ).reset_index()\n",
    "\n",
    "        # Calculate the total stats for each loan_id\n",
    "        total_summary = data.groupby('loan_id')['overdue_days'].agg(\n",
    "            sum_overdue_days='sum',\n",
    "            max_overdue_days='max'\n",
    "        ).reset_index()\n",
    "\n",
    "        # Add a column for credit_status with value 'Total' for overall stats\n",
    "        total_summary['credit_status'] = 'Total'\n",
    "\n",
    "        # Concatenate loan_summary with total_summary\n",
    "        final_summary = pd.concat([loan_summary, total_summary], axis=0).sort_values(by=['loan_id', 'credit_status']).reset_index(drop=True)\n",
    "\n",
    "        # Pivot the data so that 'credit_status' values become columns\n",
    "        pivoted_data = final_summary.pivot(index='loan_id', columns='credit_status',\n",
    "                                           values=['sum_overdue_days', 'max_overdue_days'])\n",
    "\n",
    "        # Flatten the multi-level columns\n",
    "        pivoted_data.columns = [f\"{stat}_{status}\" for stat, status in pivoted_data.columns]\n",
    "\n",
    "        # Reset the index for a cleaner DataFrame\n",
    "        pivoted_data = pivoted_data.reset_index()\n",
    "\n",
    "        return pivoted_data\n",
    "\n",
    "\n",
    "    def loan_timings():\n",
    "        # Grouping the data by 'loan_id' to calculate statistics related to 'days_since_credit'\n",
    "        result = data.groupby('loan_id')['days_since_credit'].agg(\n",
    "            days_since_first_loan_landed=lambda x: x.min(),  # Oldest loan date (minimum value of 'days_since_credit')\n",
    "            days_since_last_loan_landed=lambda x: x.max(),   # Most recent loan date (maximum value of 'days_since_credit')\n",
    "            avg_days_between_landing_loans=lambda x: x.sort_values().diff().mean()  # Average days between consecutive loans\n",
    "        ).reset_index()  # Resetting index to make 'loan_id' a column in the result\n",
    "\n",
    "\n",
    "        # Grouping data where 'days_since_end' is not null to calculate similar statistics\n",
    "        result2 = data[data[\"days_since_end\"].notna()].groupby('loan_id')['days_since_end'].agg(\n",
    "            days_since_first_loan_ended=lambda x: x.min(),  # Oldest loan end date (minimum value of 'days_since_end')\n",
    "            days_since_last_loan_ended=lambda x: x.max()    # Most recent loan end date (maximum value of 'days_since_end')\n",
    "        ).reset_index()  # Resetting index for the second result set\n",
    "\n",
    "\n",
    "        # Merging the two result sets on 'loan_id', keeping all rows from the first set\n",
    "        combined_result = pd.merge(result, result2, on='loan_id', how='left')\n",
    "\n",
    "        # Returning the final DataFrame containing the combined results\n",
    "        return combined_result\n",
    "\n",
    "\n",
    "    def historical_loans_count():\n",
    "        # Group by 'loan_id' and 'credit_status', then count occurrences\n",
    "        grouped_data = data.groupby(['loan_id', 'credit_status']).size().reset_index(name='count')\n",
    "\n",
    "        # Calculate the total count for each 'loan_id'\n",
    "        grouped_data['total_count'] = grouped_data.groupby('loan_id')['count'].transform('sum')\n",
    "\n",
    "        # Pivot the data to transform 'credit_status' values into columns\n",
    "        pivoted_data = grouped_data.pivot(index='loan_id', columns='credit_status', values='count').reset_index()\n",
    "\n",
    "        # Add the total_count column\n",
    "        pivoted_data['total_count'] = grouped_data.drop_duplicates(subset='loan_id')['total_count'].values\n",
    "\n",
    "        # Fill missing values with 0 (if any credit_status is missing for a loan_id)\n",
    "        pivoted_data = pivoted_data.fillna(0)\n",
    "\n",
    "        pivoted_data.rename(columns={'Active': 'Active_loans_count',\n",
    "                                     'Bad debt': 'Bad_loans_count',\n",
    "                                     'Closed': 'Closed_loans_count',\n",
    "                                     'Sold': 'Sold_loans_count',\n",
    "                                     \"total_count\":\"Total_loans_count\"}, inplace=True)\n",
    "\n",
    "        # Display the result\n",
    "        return pivoted_data\n",
    "\n",
    "\n",
    "    def historical_loans_types():\n",
    "        # Group by 'loan_id' and 'credit_status', then count occurrences\n",
    "        grouped_data = data.groupby(['loan_id', 'credit_type']).size().reset_index(name='count')\n",
    "\n",
    "        # Calculate the total count for each 'loan_id'\n",
    "        grouped_data['total_count'] = grouped_data.groupby('loan_id')['count'].transform('sum')\n",
    "\n",
    "        # Pivot the data to transform 'credit_status' values into columns\n",
    "        pivoted_data = grouped_data.pivot(index='loan_id', columns='credit_type', values='count').reset_index()\n",
    "\n",
    "        # Add the total_count column\n",
    "        pivoted_data['total_count'] = grouped_data.drop_duplicates(subset='loan_id')['total_count'].values\n",
    "\n",
    "        # Fill missing values with 0 (if any credit_status is missing for a loan_id)\n",
    "        pivoted_data = pivoted_data.fillna(0)\n",
    "\n",
    "        # Display the result\n",
    "        return pivoted_data\n",
    "\n",
    "\n",
    "    def calculate_loan_stats():\n",
    "        # Predefined list of columns for which statistics will be calculated\n",
    "        columns = ['credit_amount', 'debt_amount', 'credit_limit', 'overdue_amount','annuity_amount','prolong_count','max_overdue_amount']\n",
    "\n",
    "        result = data[['loan_id']].drop_duplicates()  # Start with unique loan IDs\n",
    "\n",
    "\n",
    "        for column in columns:\n",
    "            # Grouping by 'loan_id' and calculating statistics for each column\n",
    "            stats = data.groupby('loan_id')[column].agg(\n",
    "                avg_value='mean',  # Average of the column\n",
    "                max_value='max',   # Maximum value\n",
    "                min_value=lambda x: x[x > 0].min() if (x > 0).any() else x.min()   # Minimum value\n",
    "\n",
    "            ).reset_index()  # Resetting index to make 'loan_id' a regular column\n",
    "\n",
    "            # Rename columns to indicate which column they belong to\n",
    "            stats = stats.rename(columns={\n",
    "                'avg_value': f'{column}_avg',\n",
    "                'max_value': f'{column}_max',\n",
    "                'min_value': f'{column}_min',\n",
    "            })\n",
    "\n",
    "            # Merge the current stats with the result DataFrame on 'loan_id'\n",
    "            result = result.merge(stats, on='loan_id', how='left')\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Load the data and create initial DataFrame with unique loan IDs\n",
    "    data = pd.read_feather(f\"{RowData_path}creditinfo.feather\")\n",
    "    result = data[['loan_id']].drop_duplicates()\n",
    "\n",
    "    # Step 1: Calculate overdue days stats\n",
    "    overdue_data = overdue_days()\n",
    "    result = result.merge(overdue_data, on='loan_id', how='left')\n",
    "    # Comment: Merging overdue statistics to include both sum and max overdue days for different credit statuses.\n",
    "\n",
    "    # Step 2: Calculate loan timings\n",
    "    loan_timing_data = loan_timings()\n",
    "    result = result.merge(loan_timing_data, on='loan_id', how='left')\n",
    "    # Comment: Adding loan timing data to get insights on first/last loan landed and ended.\n",
    "\n",
    "    # Step 3: Calculate historical loan counts by status\n",
    "    loan_count_data = historical_loans_count()\n",
    "    result = result.merge(loan_count_data, on='loan_id', how='left')\n",
    "    # Comment: Including the count of loans by their credit status (Active, Closed, Bad Debt, Sold).\n",
    "\n",
    "    # Step 4: Calculate historical loan types\n",
    "    loan_type_data = historical_loans_types()\n",
    "    result = result.merge(loan_type_data, on='loan_id', how='left')\n",
    "    # Comment: Including the count of different loan types per loan_id.\n",
    "\n",
    "    # Step 5: Calculate loan financial statistics\n",
    "    loan_stats_data = calculate_loan_stats()\n",
    "    result = result.merge(loan_stats_data, on='loan_id', how='left')\n",
    "    # Comment: Adding financial stats like average, max, and min for different financial attributes of loans.\n",
    "\n",
    "    return result\n",
    "\n",
    "def process_documentation():\n",
    "    # Load the contacts data from a Feather file located at RowData_path\n",
    "    data = pd.read_feather(f\"{RowData_path}documentations.feather\")\n",
    "    \n",
    "    # Sum the values across the doc columns for each row\n",
    "    data['contact_mobile_count'] = data[[f\"doc_{i}\" for i in range(1, 21)] ].apply(lambda x: x.sum(), axis=1)\n",
    "    \n",
    "    return  data\n",
    "\n",
    "def process_geographical():\n",
    "    data = pd.read_feather(f\"{RowData_path}geographical.feather\")\n",
    "\n",
    "    data[\"avg_geographical_rating\"] = (data[\"region_rating\"] + data[\"city_region_rating\"])/2\n",
    "    \n",
    "    return data\n",
    "\n",
    "def process_historical_applications():\n",
    "    \n",
    "    def previous_contract_types():\n",
    "\n",
    "        # Group by 'loan_id' and 'contract_type', then count occurrences\n",
    "        grouped_data = data.groupby(['loan_id', 'contract_type']).size().reset_index(name='count')\n",
    "    \n",
    "        # Pivot the data to transform 'contract_type' values into columns\n",
    "        pivoted_data = grouped_data.pivot(index='loan_id', columns='contract_type', values='count').reset_index()\n",
    "    \n",
    "    \n",
    "        return pivoted_data\n",
    "\n",
    "\n",
    "    def previous_contract_status():\n",
    "    \n",
    "        # Group by 'loan_id' and 'contract_status', then count occurrences\n",
    "        grouped_data = data.groupby(['loan_id', 'contract_status']).size().reset_index(name='count')\n",
    "    \n",
    "        # Calculate the total count for each 'loan_id'\n",
    "        grouped_data['total_count'] = grouped_data.groupby('loan_id')['count'].transform('sum')\n",
    "    \n",
    "        # Pivot the data to transform 'contract_status' values into columns\n",
    "        pivoted_data = grouped_data.pivot(index='loan_id', columns='contract_status', values='count').reset_index()\n",
    "    \n",
    "        # Add the total_count column\n",
    "        pivoted_data['total_count'] = grouped_data.drop_duplicates(subset='loan_id')['total_count'].values\n",
    "    \n",
    "    \n",
    "        pivoted_data[\"reject_rate\"] =  (pivoted_data[\"Refused\"] / pivoted_data[\"total_count\"])*100\n",
    "        pivoted_data[\"Cancel_rate\"] =  (pivoted_data[\"Canceled\"] / pivoted_data[\"total_count\"])*100\n",
    "        pivoted_data[\"Approve_rate\"] =  (pivoted_data[\"Approved\"] / pivoted_data[\"total_count\"])*100\n",
    "    \n",
    "        return pivoted_data\n",
    "    \n",
    "    \n",
    "    def previous_contract_amounts():\n",
    "        # Group by 'loan_id' and 'contract_status', then sum and average amounts\n",
    "        grouped_data = data.groupby(['loan_id', 'contract_status']).agg(\n",
    "            amount_sum=('approved_amount', 'sum'),\n",
    "            amount_avg=('approved_amount', 'mean')\n",
    "        ).reset_index()\n",
    "    \n",
    "        # Pivot the data to transform 'contract_status' values into columns\n",
    "        pivoted_sum = grouped_data.pivot(index='loan_id', columns='contract_status', values='amount_sum').add_prefix('sum_')\n",
    "        pivoted_avg = grouped_data.pivot(index='loan_id', columns='contract_status', values='amount_avg').add_prefix('avg_')\n",
    "    \n",
    "        # Combine sum and avg pivot tables\n",
    "        pivoted_data = pivoted_sum.join(pivoted_avg).reset_index()\n",
    "    \n",
    "    \n",
    "        return pivoted_data\n",
    "\n",
    "\n",
    "\n",
    "    # Load the data and create initial DataFrame with unique loan IDs\n",
    "    data = pd.read_feather(f\"{RowData_path}hisotrical_applications.feather\")\n",
    "    result = data[['loan_id']].drop_duplicates()\n",
    "\n",
    "    # Step 1: Calculate overdue days stats\n",
    "    overdue_data = previous_contract_types()\n",
    "    result = result.merge(overdue_data, on='loan_id', how='left')\n",
    "    # Comment: Merging overdue statistics to include both sum and max overdue days for different credit statuses.\n",
    "\n",
    "    # Step 2: Calculate loan timings\n",
    "    loan_timing_data = previous_contract_status()\n",
    "    result = result.merge(loan_timing_data, on='loan_id', how='left')\n",
    "    # Comment: Adding loan timing data to get insights on first/last loan landed and ended.\n",
    "\n",
    "    # Step 3: Calculate historical loan counts by status\n",
    "    loan_count_data = previous_contract_amounts()\n",
    "    result = result.merge(loan_count_data, on='loan_id', how='left')\n",
    "    # Comment: Including the count of loans by their credit status (Active, Closed, Bad Debt, Sold).\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "def process_personal():\n",
    "    data = pd.read_feather(f\"{RowData_path}personal.feather\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def process_residence():\n",
    "    data = pd.read_feather(f\"{RowData_path}residence.feather\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def process_credit_card_balance():\n",
    "    data = pd.read_feather(f\"{RowData_path}credit_card_balance.feather\")\n",
    "    \n",
    "    # Overall history\n",
    "    grouped_data_total = data.groupby(['loan_id']).agg(\n",
    "        max_credit_limit=('credit_limit', 'max'),\n",
    "        total_atm_draw_amount=('atm_draw_amount', 'sum'),\n",
    "        total_draw_amount=('draw_amount', 'sum'),\n",
    "        total_other_draw_amount=('other_draw_amount', 'sum'),\n",
    "        total_pos_draw_amount=('pos_draw_amount', 'sum'),\n",
    "        total_atm_draw_count=('atm_draw_count', 'sum'),\n",
    "        total_other_draw_count=('other_draw_count', 'sum'),\n",
    "        total_pos_draw_count=('pos_draw_count', 'sum'),\n",
    "        total_paid_installments=('paid_installments', 'sum'),\n",
    "        max_dpd=('other_draw_count', 'max'),\n",
    "    ).reset_index()\n",
    "\n",
    "    # Last 12 months\n",
    "    grouped_data_last_12_months = data[data[\"balance_month\"] >= -12].groupby(['loan_id']).agg(\n",
    "        max_credit_limit=('credit_limit', 'max'),\n",
    "        total_atm_draw_amount=('atm_draw_amount', 'sum'),\n",
    "        total_draw_amount=('draw_amount', 'sum'),\n",
    "        total_other_draw_amount=('other_draw_amount', 'sum'),\n",
    "        total_pos_draw_amount=('pos_draw_amount', 'sum'),\n",
    "        total_atm_draw_count=('atm_draw_count', 'sum'),\n",
    "        total_other_draw_count=('other_draw_count', 'sum'),\n",
    "        total_pos_draw_count=('pos_draw_count', 'sum'),\n",
    "        total_paid_installments=('paid_installments', 'sum'),\n",
    "        max_dpd=('other_draw_count', 'max'),\n",
    "    ).reset_index()\n",
    "\n",
    "    # Last 6 months\n",
    "    grouped_data_last_6_months = data[data[\"balance_month\"] >= -6].groupby(['loan_id']).agg(\n",
    "        max_credit_limit=('credit_limit', 'max'),\n",
    "        total_atm_draw_amount=('atm_draw_amount', 'sum'),\n",
    "        total_draw_amount=('draw_amount', 'sum'),\n",
    "        total_other_draw_amount=('other_draw_amount', 'sum'),\n",
    "        total_pos_draw_amount=('pos_draw_amount', 'sum'),\n",
    "        total_atm_draw_count=('atm_draw_count', 'sum'),\n",
    "        total_other_draw_count=('other_draw_count', 'sum'),\n",
    "        total_pos_draw_count=('pos_draw_count', 'sum'),\n",
    "        total_paid_installments=('paid_installments', 'sum'),\n",
    "        max_dpd=('other_draw_count', 'max'),\n",
    "    ).reset_index()\n",
    "\n",
    "\n",
    "    # Rename columns for clarity before merging\n",
    "    grouped_data_total = grouped_data_total.rename(\n",
    "        columns={col: f\"{col}_total\" for col in grouped_data_total.columns if col != 'loan_id'}\n",
    "    )\n",
    "\n",
    "    grouped_data_last_12_months = grouped_data_last_12_months.rename(\n",
    "        columns={col: f\"{col}_last_12m\" for col in grouped_data_last_12_months.columns if col != 'loan_id'}\n",
    "    )\n",
    "\n",
    "    grouped_data_last_6_months = grouped_data_last_6_months.rename(\n",
    "        columns={col: f\"{col}_last_6m\" for col in grouped_data_last_6_months.columns if col != 'loan_id'}\n",
    "    )\n",
    "\n",
    "    # Merge overall data with last 12 months data\n",
    "    combined_data = grouped_data_total.merge(\n",
    "        grouped_data_last_12_months,\n",
    "        on='loan_id',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Merge the result with last 6 months data\n",
    "    combined_data = combined_data.merge(\n",
    "        grouped_data_last_6_months,\n",
    "        on='loan_id',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    combined_data[\"have_credit_card\"] = \"Yes\"\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "def process_pos_cash_balance():\n",
    "    data = pd.read_feather(f\"{RowData_path}pos_cash_balance.feather\")\n",
    "\n",
    "    filtered_data = data[data[\"balance_month\"] == -1].dropna()\n",
    "\n",
    "    grouped_data_with_max = filtered_data.groupby('loan_id').agg(\n",
    "        sum_future_installments = ('future_installments', 'sum'),\n",
    "        max_future_installments = ('future_installments', 'sum'),\n",
    "        max_dpd= ('dpd', 'sum'),\n",
    "        max_dpd_tolerance= ('dpd_tolerance', 'sum'),\n",
    "        sum_dpd = ('dpd', 'sum'),\n",
    "        sum_dpd_tolerance= ('dpd_tolerance', 'sum'),\n",
    "    ).reset_index()\n",
    "\n",
    "    return grouped_data_with_max"
   ],
   "id": "2fe8342d7d3576ff",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T17:47:45.671426Z",
     "start_time": "2024-11-14T17:43:39.397998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "applications = process_applications()\n",
    "contacts = process_contacts()\n",
    "credit_info = process_credit_info()\n",
    "documentation = process_documentation()\n",
    "geographical = process_geographical()\n",
    "historical_applications = process_historical_applications()\n",
    "personal = process_personal()\n",
    "residence = process_residence()\n",
    "credit_card_balance = process_credit_card_balance()\n",
    "pos_cash_balance = process_pos_cash_balance()"
   ],
   "id": "16726e42fd057f0d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T17:47:54.493337Z",
     "start_time": "2024-11-14T17:47:52.250765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dfs = [\n",
    "    contacts, credit_info, documentation, geographical,\n",
    "    historical_applications, personal, residence,\n",
    "    credit_card_balance, pos_cash_balance\n",
    "]\n",
    "\n",
    "# Start with the applications dataframe\n",
    "merged_df = applications\n",
    "\n",
    "# Perform successive left joins\n",
    "for df in dfs:\n",
    "    merged_df = merged_df.merge(df, on='loan_id', how='left')"
   ],
   "id": "9b245ef717b70710",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T17:49:43.608763Z",
     "start_time": "2024-11-14T17:49:43.068190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "merged_df.to_feather(RowData_path + \"ML_data.feather\")"
   ],
   "id": "1cd10224fdeb2a47",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [],
   "id": "87a8455f52cf1309"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
